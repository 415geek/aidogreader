# -*- coding: utf-8 -*-
# app.py â€” Dog Behavior & Affect Analyzer (Production-Ready Skeleton)
# åŠŸèƒ½ï¼šä¸Šä¼ è§†é¢‘ -> æ£€æµ‹è¿½è¸ª -> ç‰¹å¾æå– -> è¡Œä¸ºè¯†åˆ«(å¯å¢é‡å­¦ä¹ ) -> æƒ…ç»ªæ˜ å°„ -> æ—¶é—´è½´æŠ¥å‘Š/å¯è§†åŒ–
# ä¾èµ–ï¼šstreamlit, ultralytics, opencv-python, librosa, soundfile, scikit-learn, joblib, numpy

import os, io, json, time, uuid, math, tempfile
from dataclasses import dataclass
from typing import List, Tuple, Dict, Any

import numpy as np
import streamlit as st
import cv2
from ultralytics import YOLO
import librosa
import soundfile as sf
from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import accuracy_score, f1_score
import joblib

# ------------ å…¨å±€é…ç½® ------------
APP_TITLE = "ğŸ¶ Dog Behavior & Affect Analyzer"
DATA_DIR = "data_samples"
MODEL_DIR = "models"
REPORT_DIR = "reports"
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(REPORT_DIR, exist_ok=True)

LABELS = ["lying", "sitting/idle", "walking", "running", "sprinting/jumping"]
AFFECT_TABLE = {
    "lying": (0.20, 0.70),           # (arousal, valence)
    "sitting/idle": (0.30, 0.60),
    "walking": (0.45, 0.65),
    "running": (0.70, 0.65),
    "sprinting/jumping": (0.85, 0.60),
}
CLASS_COLORS = {
    "lying": (120, 200, 80),
    "sitting/idle": (200, 200, 80),
    "walking": (80, 180, 220),
    "running": (80, 120, 240),
    "sprinting/jumping": (60, 60, 255),
}

# ------------ æ•°æ®ç»“æ„ ------------
@dataclass
class Segment:
    seg_id: str
    t_start: float
    t_end: float
    features: np.ndarray
    auto_label: str
    auto_conf: float
    bark: bool

# ------------ æ¨¡å‹åŠ è½½ ------------
@st.cache_resource
def load_detector():
    # è½»é‡æ¨¡å‹å³å¯ï¼Œé¦–æ¬¡è‡ªåŠ¨ä¸‹è½½
    return YOLO("yolov8n.pt")

@st.cache_resource
def init_or_load_clf():
    model_p = os.path.join(MODEL_DIR, "behavior_clf_latest.joblib")
    scaler_p = os.path.join(MODEL_DIR, "scaler_latest.joblib")
    if os.path.exists(model_p) and os.path.exists(scaler_p):
        clf = joblib.load(model_p)
        scaler = joblib.load(scaler_p)
    else:
        # åˆå§‹åŒ–ï¼ˆå…ˆç”¨è§„åˆ™ç”Ÿæˆçš„â€œä¼ªæ ‡ç­¾â€åšå†·å¯ï¼Œåç»­é€šè¿‡ä¸»åŠ¨å­¦ä¹ å¢é‡ä¿®æ­£ï¼‰
        base = SGDClassifier(loss="log_loss", alpha=1e-4, learning_rate="optimal", random_state=42)
        clf = CalibratedClassifierCV(base_estimator=base, method="sigmoid", cv=3)
        scaler = StandardScaler(with_mean=True, with_std=True)
        # å°šæœªæ‹Ÿåˆï¼Œä½†å…ˆå­˜â€œç©ºå£³â€ï¼Œé¿å…é¦–æ¬¡è°ƒç”¨æŠ¥é”™
        joblib.dump(clf, model_p); joblib.dump(scaler, scaler_p)
    return clf, scaler

# ------------ å·¥å…·å‡½æ•° ------------
def iou(a, b):
    xA, yA = max(a[0], b[0]), max(a[1], b[1])
    xB, yB = min(a[2], b[2]), min(a[3], b[3])
    inter = max(0, xB - xA) * max(0, yB - yA)
    areaA = (a[2]-a[0])*(a[3]-a[1]); areaB = (b[2]-b[0])*(b[3]-b[1])
    union = areaA + areaB - inter + 1e-6
    return inter / union

def extract_audio_signal(video_path, target_sr=16000):
    try:
        y, sr = librosa.load(video_path, sr=target_sr, mono=True)
        return y, sr
    except Exception:
        return None, None

def bark_score_track(y, sr, frame_ms=400, hop_ms=160) -> List[Tuple[float, float, float]]:
    """è¿”å›[(start_s, end_s, bark_score)]ã€‚bark_score=èƒ½é‡*é«˜é¢‘å æ¯”çš„è§„èŒƒåŒ–åˆ†æ•°ã€‚"""
    if y is None: return []
    frame_len = int(frame_ms/1000*sr); hop_len = int(hop_ms/1000*sr)
    out = []
    i = 0
    # åŠ¨æ€é˜ˆå€¼ï¼šåŸºäºæ•´æ®µ RMS
    rms = np.sqrt(np.mean(y**2) + 1e-9)
    base_e = max(1e-6, rms**2)
    while i + frame_len <= len(y):
        seg = y[i:i+frame_len]
        energy = float(np.mean(seg**2) / base_e)  # ç›¸å¯¹èƒ½é‡
        S = np.abs(librosa.stft(seg, n_fft=512, hop_length=128))
        freqs = librosa.fft_frequencies(sr=sr, n_fft=512)
        mask = (freqs >= 700) & (freqs <= 3500)  # çŠ¬å ä¸»èƒ½å¸¦ï¼ˆç²—ç•¥ï¼‰
        ratio = (np.sum(S[mask]) + 1e-6) / (np.sum(S) + 1e-6)
        score = float(energy * ratio)
        out.append((i/sr, (i+frame_len)/sr, score))
        i += hop_len
    return out

def rule_behavior(speed_px, aspect_ratio, area_change):
    # é€Ÿåº¦ä¸»å¯¼ï¼Œå½¢æ€ä¿®æ­£ï¼ˆèººä¸‹æ—¶é•¿å®½æ¯”åå°/é¢ç§¯æ³¢åŠ¨å°ï¼‰
    if speed_px < 2.0:
        if aspect_ratio < 0.85 and area_change < 0.01:
            return "lying", 0.70
        return "sitting/idle", 0.60
    elif speed_px < 10.0:
        return "walking", 0.70
    elif speed_px < 23.0:
        return "running", 0.75
    else:
        return "sprinting/jumping", 0.80

def affect_from_behavior(label:str, bark:bool):
    a, v = AFFECT_TABLE.get(label, (0.5, 0.5))
    if bark: a = min(1.0, a + 0.1)
    conf_aff = 0.45 if label in ["lying","sitting/idle"] else 0.55
    return a, v, conf_aff

def save_sample(features: np.ndarray, true_label: str, meta: dict):
    sid = str(uuid.uuid4())
    np.save(os.path.join(DATA_DIR, f"{sid}_x.npy"), features.astype(np.float32))
    json.dump({"y": true_label, "meta": meta}, open(os.path.join(DATA_DIR, f"{sid}_y.json"), "w"))

def load_samples(limit=None):
    xs, ys = [], []
    files = [f for f in os.listdir(DATA_DIR) if f.endswith("_y.json")]
    if limit: files = files[:limit]
    for jf in files:
        meta = json.load(open(os.path.join(DATA_DIR, jf)))
        y = meta["y"]
        sid = jf.replace("_y.json","")
        x = np.load(os.path.join(DATA_DIR, f"{sid}_x.npy"))
        xs.append(x); ys.append(y)
    if not xs: return None, None
    X = np.vstack(xs)
    y = np.array([LABELS.index(v) for v in ys], dtype=np.int64)
    return X, y

def fit_or_partial_update(X_train, y_train):
    # å…¨é‡å°è®­ç»ƒï¼ˆæ›´ç¨³ï¼‰ï¼Œä½ ä¹Ÿå¯ä»¥æŠŠ base_clf.partial_fit åšæˆçœŸÂ·åœ¨çº¿
    scaler = StandardScaler(with_mean=True, with_std=True)
    scaler.fit(X_train)
    Xs = scaler.transform(X_train)
    base = SGDClassifier(loss="log_loss", alpha=1e-4, learning_rate="optimal", random_state=42)
    classes = np.arange(len(LABELS))
    base.partial_fit(Xs, y_train, classes=classes)
    calibrated = CalibratedClassifierCV(base_estimator=base, method="sigmoid", cv=3)
    calibrated.fit(Xs, y_train)
    ts = time.strftime("%Y%m%d_%H%M%S")
    joblib.dump(calibrated, os.path.join(MODEL_DIR, f"behavior_clf_{ts}.joblib"))
    joblib.dump(scaler, os.path.join(MODEL_DIR, f"scaler_{ts}.joblib"))
    joblib.dump(calibrated, os.path.join(MODEL_DIR, "behavior_clf_latest.joblib"))
    joblib.dump(scaler, os.path.join(MODEL_DIR, "scaler_latest.joblib"))
    return calibrated, scaler, ts

def predict_with_model(features_vec: np.ndarray):
    model_p = os.path.join(MODEL_DIR, "behavior_clf_latest.joblib")
    scaler_p = os.path.join(MODEL_DIR, "scaler_latest.joblib")
    if not (os.path.exists(model_p) and os.path.exists(scaler_p)):
        return None
    clf = joblib.load(model_p)
    scaler = joblib.load(scaler_p)
    Xs = scaler.transform(features_vec.reshape(1, -1))
    probs = clf.predict_proba(Xs)[0]
    idx = int(np.argmax(probs))
    return LABELS[idx], float(probs[idx])

# ------------ Streamlit UI ------------
st.set_page_config(page_title=APP_TITLE, layout="centered")
st.title(APP_TITLE)
st.caption("ä¸Šä¼ ä¸€æ®µçŸ­è§†é¢‘ï¼Œç³»ç»Ÿå°†è¾“å‡ºï¼šè¡Œä¸ºè¯†åˆ«ã€ç½®ä¿¡åº¦ã€å å«æ¦‚ç‡ä»¥åŠæƒ…ç»ªï¼ˆå”¤é†’/æ•ˆä»·ï¼‰æ¨æ–­ï¼Œå¹¶æ”¯æŒè¾¹ç”¨è¾¹å­¦ã€‚")

with st.sidebar:
    st.header("å‚æ•°")
    max_seconds = st.slider("åˆ†ææ—¶é•¿ä¸Šé™(ç§’)", 5, 90, 25)
    conf_th = st.slider("æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆYOLOï¼‰", 0.1, 0.8, 0.35)
    sample_fps = st.slider("åˆ†ææŠ½å¸§é€Ÿç‡(fps)", 3, 12, 6)
    bark_th = st.slider("å å«åˆ†æ•°é˜ˆå€¼", 0.35, 2.0, 0.65)
    lowconf_th = st.slider("ä½ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆè§¦å‘æ ‡æ³¨ï¼‰", 0.50, 0.90, 0.65)
    st.markdown("---")
    do_train = st.button("ğŸ§  ä½¿ç”¨å·²æ ‡æ³¨æ ·æœ¬æ”¹è¿›æ¨¡å‹")

uploaded = st.file_uploader("ä¸Šä¼ è§†é¢‘ (mp4/mov/mkv)", type=["mp4","mov","mkv"])

if do_train:
    X_all, y_all = load_samples()
    if X_all is None:
        st.warning("æš‚æ— æ ‡æ³¨æ ·æœ¬ã€‚å…ˆåœ¨ä¸‹æ–¹æ—¶é—´è½´ä¸­ä¿å­˜å‡ æ¡è®­ç»ƒæ ·æœ¬ã€‚")
    else:
        _, _, tag = fit_or_partial_update(X_all, y_all)
        st.success(f"æ¨¡å‹å·²æ›´æ–° âœ…ï¼ˆç‰ˆæœ¬ {tag) }ï¼‰")

if uploaded:
    # ç¼“å­˜ä¸´æ—¶è§†é¢‘
    tmpf = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
    tmpf.write(uploaded.read()); tmpf.close()

    # éŸ³é¢‘ â†’ å å«åˆ†æ•°è½¨è¿¹
    audio, sr = extract_audio_signal(tmpf.name)
    bark_track = bark_score_track(audio, sr) if sr else []
    def bark_present(t0, t1):
        if not bark_track: return False, 0.0
        scores = [s for (a,b,s) in bark_track if not (b <= t0 or a >= t1)]
        if not scores: return False, 0.0
        smax = float(np.max(scores))
        return (smax >= bark_th), smax

    # åŠ è½½æ£€æµ‹å™¨
    det = load_detector()
    cap = cv2.VideoCapture(tmpf.name)
    fps = cap.get(cv2.CAP_PROP_FPS) or 30
    total_frames = int(min(cap.get(cv2.CAP_PROP_FRAME_COUNT), max_seconds*fps))
    step = max(1, int(round(fps / sample_fps)))
    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)); H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    st.info("å¼€å§‹åˆ†æï¼šä¸ºæé€Ÿå°†æŠ½å¸§å¤„ç†ï¼ˆä¸å½±å“æœ€ç»ˆåˆ¤æ–­çš„ç¨³å®šæ€§ï¼‰ã€‚")
    progress = st.progress(0)
    last_box = None
    last_area = None
    segments: List[Segment] = []

    # æ—¶çª—èšåˆï¼ˆæŒ‰ N å¸§ä¸ºä¸€å°æ®µï¼‰
    window_frames = max(3, int(sample_fps * 1.2))  # ~1.2s
    buf_feats, buf_times = [], []

    for idx in range(0, total_frames, step):
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if not ret: break
        t_sec = idx / fps

        # YOLO æ¨ç†
        res = det(frame, conf=conf_th, verbose=False)[0]
        dog_boxes = []
        for b in res.boxes:
            cls = int(b.cls[0].item())
            if det.model.names.get(cls, "") == "dog":
                x1,y1,x2,y2 = b.xyxy[0].cpu().numpy().astype(int).tolist()
                dog_boxes.append([x1,y1,x2,y2, float(b.conf[0].item())])

        if dog_boxes:
            # å–æœ€å¤§æ¡†ï¼ˆå¤šçŠ¬åœºæ™¯é»˜è®¤ä¸»è§’ï¼‰ï¼Œäº¦å¯æ”¹æˆå¤šç›®æ ‡
            dog_boxes.sort(key=lambda x: (x[2]-x[0])*(x[3]-x[1]), reverse=True)
            box = dog_boxes[0][:4]

            # é€Ÿåº¦/åŠ é€Ÿåº¦/å½¢æ€ç‰¹å¾
            cx, cy = (box[0]+box[2])/2, (box[1]+box[3])/2
            w_box, h_box = box[2]-box[0], box[3]-box[1]
            area = w_box * h_box
            aspect = min(w_box, h_box) / (max(w_box, h_box)+1e-6)

            speed = 0.0; acc = 0.0; area_chg = 0.0
            if last_box is not None and iou(last_box, box) > 0.1:
                lx, ly = (last_box[0]+last_box[2])/2, (last_box[1]+last_box[3])/2
                speed = math.hypot(cx-lx, cy-ly)  # åƒç´ /æ­¥
                if last_area is not None and last_area > 0:
                    area_chg = abs(area - last_area) / (last_area + 1e-6)
            last_box, last_area = box, area

            # ç®€å•ä¸€é˜¶å·®åˆ†å¾—åˆ°â€œåŠ é€Ÿåº¦â€çš„ä»£ç†ï¼ˆç”¨é€Ÿåº¦ä¸ä¸Šä¸€æ¬¡é€Ÿåº¦çš„å·®ï¼‰
            if buf_feats:
                prev_speed = buf_feats[-1][0]
                acc = max(0.0, speed - prev_speed)

            # æ±‡å…¥ç¼“å†²
            features_vec = np.array([
                speed, acc, aspect, area/(W*H+1e-6), area_chg
            ], dtype=np.float32)
            buf_feats.append((speed, acc, aspect, area/(W*H+1e-6), area_chg))
            buf_times.append(t_sec)

            # çª—å£èšåˆæˆç‰‡æ®µ
            if len(buf_feats) >= window_frames:
                f = np.array(buf_feats[-window_frames:], dtype=np.float32)
                # ç»Ÿè®¡ç‰¹å¾ï¼ˆå‡å€¼ã€æ–¹å·®ã€æœ€å¤§å€¼ï¼‰
                agg = np.concatenate([
                    f.mean(axis=0),
                    f.std(axis=0),
                    f.max(axis=0),
                ], axis=0)  # 5ç»´*3=15ç»´
                t0, t1 = buf_times[-window_frames], buf_times[-1]

                # è§„åˆ™é¢„æµ‹ï¼ˆä½œä¸ºå†·å¯ä¸å¤‡é€‰ï¼‰
                rb_label, rb_conf = rule_behavior(
                    speed_px=float(f[:,0].mean()),
                    aspect_ratio=float(f[:,2].mean()),
                    area_change=float(f[:,4].mean())
                )

                # ç›‘ç£æ¨¡å‹é¢„æµ‹ï¼ˆè‹¥å·²æœ‰ï¼‰
                mdl_pred = predict_with_model(agg)
                if mdl_pred is not None:
                    auto_label, auto_conf = mdl_pred
                    # è‹¥æ¨¡å‹ç½®ä¿¡ä½äºè§„åˆ™ï¼Œé‡‡ç”¨è§„åˆ™ç»“æœå¢ç¨³
                    if auto_conf < 0.55 and rb_conf > auto_conf:
                        auto_label, auto_conf = rb_label, rb_conf
                else:
                    auto_label, auto_conf = rb_label, rb_conf

                # å å«åˆ¤æ–­
                bark, bark_s = bark_present(t0, t1)

                seg = Segment(
                    seg_id=str(uuid.uuid4()),
                    t_start=float(t0), t_end=float(t1),
                    features=agg, auto_label=auto_label, auto_conf=auto_conf,
                    bark=bool(bark)
                )
                segments.append(seg)

        progress.progress(min(1.0, (idx+step)/max(1,total_frames)))

    cap.release()

    # ------- æŠ¥å‘Šä¸æ ‡æ³¨ UI -------
    if not segments:
        st.error("æœªæ£€æµ‹åˆ°ç‹—ã€‚è¯·ç¡®ä¿ç”»é¢ä¸­æœ‰æ¸…æ™°çš„ç‹—å¹¶æœ‰è¶³å¤Ÿè¿åŠ¨ã€‚")
        st.stop()

    st.subheader("åˆ†æç»“æœï¼ˆæ—¶é—´è½´ï¼‰")
    rows = []
    for s in segments:
        a,v,aff_c = affect_from_behavior(s.auto_label, s.bark)
        rows.append({
            "start(s)": round(s.t_start,2),
            "end(s)": round(s.t_end,2),
            "behavior": s.auto_label,
            "conf": round(s.auto_conf,2),
            "bark": "yes" if s.bark else "no",
            "arousal": round(a,2),
            "valence": round(v,2)
        })
    st.dataframe(rows, use_container_width=True)

    # ä½ç½®ä¿¡åº¦ â†’ ä¸»åŠ¨å­¦ä¹ æ ‡æ³¨
    st.subheader("éœ€è¦ä½ æ¥æ•™ä¸€æ•™ï¼ˆä½ç½®ä¿¡åº¦ç‰‡æ®µï¼‰")
    n_flag = 0
    for s in segments:
        if s.auto_conf < lowconf_th:
            n_flag += 1
            with st.expander(f"{s.t_start:.2f}â€“{s.t_end:.2f}s  æ¨¡å‹ï¼š{s.auto_label}ï¼ˆ{s.auto_conf:.2f}ï¼‰ | å å«={'æ˜¯' if s.bark else 'å¦'}"):
                choice = st.selectbox("çœŸå®è¡Œä¸ºæ ‡ç­¾", LABELS, index=LABELS.index(s.auto_label),
                                      key=f"sel_{s.seg_id}")
                if st.button("ä¿å­˜ä¸ºè®­ç»ƒæ ·æœ¬", key=f"save_{s.seg_id}"):
                    meta = {"t0": s.t_start, "t1": s.t_end, "bark": s.bark}
                    save_sample(s.features, choice, meta)
                    st.success("æ ·æœ¬å·²ä¿å­˜ âœ… ä¸‹æ¬¡ç‚¹å‡»ä¾§æ â€œæ”¹è¿›æ¨¡å‹â€å³å¯å­¦ä¹ ã€‚")
    if n_flag == 0:
        st.caption("æ‰€æœ‰ç‰‡æ®µç½®ä¿¡åº¦éƒ½ä¸é”™ï¼Œæ— éœ€æ ‡æ³¨ã€‚")

    # å¿«é€ŸæŠ¥å‘Šå¯¼å‡ºï¼ˆJSONï¼‰
    if st.button("ğŸ“„ å¯¼å‡ºæœ¬æ¬¡æŠ¥å‘Š(JSON)"):
        report = {
            "video": uploaded.name,
            "segments": [
                {
                    "t_start": s.t_start, "t_end": s.t_end,
                    "behavior": s.auto_label, "conf": s.auto_conf,
                    "bark": s.bark,
                    "arousal": affect_from_behavior(s.auto_label, s.bark)[0],
                    "valence": affect_from_behavior(s.auto_label, s.bark)[1],
                } for s in segments
            ],
            "created_at": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        rid = time.strftime("%Y%m%d_%H%M%S")
        path = os.path.join(REPORT_DIR, f"report_{rid}.json")
        with open(path, "w") as f:
            json.dump(report, f, indent=2)
        st.success(f"å·²å¯¼å‡ºï¼š{path}")

    st.markdown("---")
    st.caption("æç¤ºï¼šè¦æ˜¾è‘—æå‡å‡†ç¡®ç‡ï¼Œè¯·ç´¯ç§¯ä½ çš„çœŸå®åœºæ™¯æ ‡æ³¨æ ·æœ¬ï¼Œç„¶ååœ¨ä¾§æ æ‰§è¡Œâ€œæ”¹è¿›æ¨¡å‹â€ã€‚ä¹Ÿå¯ä»¥æ›¿æ¢ YOLO æƒé‡ä¸ºä½ çš„è‡ªè®­æ¨¡å‹ï¼Œæˆ–æ¥å…¥åŠ¨ç‰©å§¿æ€ä¼°è®¡ï¼ˆDLC/SLEAPï¼‰è¿›ä¸€æ­¥ç»†åŒ–â€œå/è¶´/æ‘‡å°¾/æŠ“æŒ /èˆ”çˆªâ€ç­‰åŸå­è¡Œä¸ºã€‚")
