# -*- coding: utf-8 -*-
# Dog Behavior & Affect Analyzer â€” Pro edition (Cloud-safe)
# - Lazy import Ultralytics; OpenCV headless safe
# - Simple/Pro sidebar; Evidence cards (head micro-expressions & tail)
# - Natural language summary + JSON & TXT report export
# - Optional online learning via sklearn (if installed)

import os, json, time, uuid, math, tempfile
from dataclasses import dataclass
from typing import List, Tuple, Optional

import numpy as np
import streamlit as st

# -------------------- Safe OpenCV import --------------------
try:
    import cv2
except Exception as e:
    cv2 = None
    CV2_IMPORT_ERR = e
else:
    CV2_IMPORT_ERR = None

# -------------------- Optional sklearn --------------------
SK_OK = True
try:
    from sklearn.linear_model import SGDClassifier
    from sklearn.preprocessing import StandardScaler
    from sklearn.calibration import CalibratedClassifierCV
except Exception:
    SK_OK = False

import joblib

# -------------------- App config --------------------
APP_TITLE = "ğŸ¶ Dog Behavior & Affect Analyzer"
DATA_DIR = "data_samples"; MODEL_DIR = "models"; REPORT_DIR = "reports"
os.makedirs(DATA_DIR, exist_ok=True); os.makedirs(MODEL_DIR, exist_ok=True); os.makedirs(REPORT_DIR, exist_ok=True)

LABELS = ["lying", "sitting/idle", "walking", "running", "sprinting/jumping"]
AFFECT_TABLE = {
    "lying": (0.20, 0.70),
    "sitting/idle": (0.30, 0.60),
    "walking": (0.45, 0.65),
    "running": (0.70, 0.65),
    "sprinting/jumping": (0.85, 0.60),
}

# -------------------- Dataclass --------------------
@dataclass
class Segment:
    seg_id: str
    t_start: float
    t_end: float
    features: np.ndarray
    auto_label: str
    auto_conf: float
    bark: bool  # audio disabled for cloud

# -------------------- Utils --------------------
def iou(a, b):
    xA, yA = max(a[0], b[0]), max(a[1], b[1])
    xB, yB = min(a[2], b[2]), min(a[3], b[3])
    inter = max(0, xB - xA) * max(0, yB - yA)
    areaA = (a[2]-a[0])*(a[3]-a[1]); areaB = (b[2]-b[0])*(b[3]-b[1])
    return inter / (areaA + areaB - inter + 1e-6)

def rule_behavior(speed_px: float, aspect_ratio: float, area_change: float) -> Tuple[str, float]:
    if speed_px < 2.0:
        if aspect_ratio < 0.85 and area_change < 0.01: return "lying", 0.70
        return "sitting/idle", 0.60
    elif speed_px < 10.0: return "walking", 0.70
    elif speed_px < 23.0: return "running", 0.75
    else: return "sprinting/jumping", 0.80

def affect_from_behavior(label: str, bark: bool) -> Tuple[float, float, float]:
    a, v = AFFECT_TABLE.get(label, (0.5, 0.5))
    conf_aff = 0.45 if label in ["lying","sitting/idle"] else 0.55
    return a, v, conf_aff

# --- ROI & micro features ---
def crop_roi(frame, box, rel):
    x1, y1, x2, y2 = box
    w, h = x2 - x1, y2 - y1
    rx1 = int(x1 + rel[0] * w); ry1 = int(y1 + rel[1] * h)
    rx2 = int(x1 + rel[2] * w); ry2 = int(y1 + rel[3] * h)
    rx1, ry1 = max(0, rx1), max(0, ry1)
    rx2, ry2 = min(frame.shape[1], rx2), min(frame.shape[0], ry2)
    if rx2 - rx1 < 4 or ry2 - ry1 < 4: return None
    return frame[ry1:ry2, rx1:rx2].copy()

def tail_wag_features(prev_tail_gray, tail_gray):
    """å°¾å·´ ROI å…‰æµè¿‘ä¼¼ç‰¹å¾ï¼ˆå¢åŠ é˜²é”™æœºåˆ¶ï¼‰"""
    # æ£€æŸ¥è¾“å…¥æœ‰æ•ˆæ€§
    if prev_tail_gray is None or tail_gray is None:
        return None
    if not isinstance(prev_tail_gray, np.ndarray) or not isinstance(tail_gray, np.ndarray):
        return None
    if prev_tail_gray.shape != tail_gray.shape:
        try:
            tail_gray = cv2.resize(tail_gray, (prev_tail_gray.shape[1], prev_tail_gray.shape[0]))
        except Exception:
            return None
    try:
        diff = cv2.absdiff(tail_gray, prev_tail_gray)
    except Exception:
        return None

    mag = float(np.mean(diff))  # æ‘†åŠ¨å¼ºåº¦è¿‘ä¼¼
    gx = cv2.Sobel(tail_gray, cv2.CV_32F, 1, 0, ksize=3)
    gy = cv2.Sobel(tail_gray, cv2.CV_32F, 0, 1, ksize=3)
    ori_ratio = float(np.mean(np.abs(gx))) / (np.mean(np.abs(gy)) + 1e-6)
    return {"wag_mag": mag, "wag_orient": ori_ratio}
def head_micro_features(head_bgr):
    if head_bgr is None: return None
    img = cv2.resize(head_bgr, (128, 128))
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5,5), 0)
    eye_roi = blur[20:64, 32:96]
    mouth_roi = blur[72:120, 24:104]
    ear_left  = blur[8:40,  0:40]
    ear_right = blur[8:40, 88:128]
    _, eye_th = cv2.threshold(eye_roi, 0, 255, cv2.THRESH_OTSU)
    eye_open = 1.0 - (np.mean(eye_th)/255.0)
    edges = cv2.Canny(mouth_roi, 50, 120)
    mouth_open = float(np.mean(edges > 0))
    ear_l_edge = float(np.mean(cv2.Canny(ear_left, 50, 120) > 0))
    ear_r_edge = float(np.mean(cv2.Canny(ear_right, 50, 120) > 0))
    ear_up = (ear_l_edge + ear_r_edge) / 2.0
    return {"eye_open": eye_open, "mouth_open": mouth_open, "ear_up": ear_up}

# -------------------- Samples I/O --------------------
def save_sample(features: np.ndarray, true_label: str, meta: dict):
    sid = str(uuid.uuid4())
    np.save(os.path.join(DATA_DIR, f"{sid}_x.npy"), features.astype(np.float32))
    with open(os.path.join(DATA_DIR, f"{sid}_y.json"), "w") as f:
        json.dump({"y": true_label, "meta": meta}, f)

def load_samples(limit: Optional[int] = None):
    files = [f for f in os.listdir(DATA_DIR) if f.endswith("_y.json")]
    if not files: return None, None
    if limit: files = files[:limit]
    Xs, ys = [], []
    for jf in files:
        meta = json.load(open(os.path.join(DATA_DIR, jf)))
        y = meta["y"]; sid = jf.replace("_y.json", "")
        x = np.load(os.path.join(DATA_DIR, f"{sid}_x.npy"))
        Xs.append(x); ys.append(LABELS.index(y))
    return np.vstack(Xs), np.array(ys, dtype=np.int64)

def fit_or_partial_update(X_train: np.ndarray, y_train: np.ndarray):
    if not SK_OK: return None, None, None
    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_train)
    Xs = scaler.transform(X_train)
    base = SGDClassifier(loss="log_loss", alpha=1e-4, learning_rate="optimal", random_state=42)
    classes = np.arange(len(LABELS)); base.partial_fit(Xs, y_train, classes=classes)
    clf = CalibratedClassifierCV(base_estimator=base, method="sigmoid", cv=3).fit(Xs, y_train)
    ts = time.strftime("%Y%m%d_%H%M%S")
    joblib.dump(clf, os.path.join(MODEL_DIR, f"behavior_clf_{ts}.joblib"))
    joblib.dump(scaler, os.path.join(MODEL_DIR, f"scaler_{ts}.joblib"))
    joblib.dump(clf, os.path.join(MODEL_DIR, "behavior_clf_latest.joblib"))
    joblib.dump(scaler, os.path.join(MODEL_DIR, "scaler_latest.joblib"))
    return clf, scaler, ts

def predict_with_model(features_vec: np.ndarray):
    if not SK_OK: return None
    mp, sp = os.path.join(MODEL_DIR, "behavior_clf_latest.joblib"), os.path.join(MODEL_DIR, "scaler_latest.joblib")
    if not (os.path.exists(mp) and os.path.exists(sp)): return None
    clf, scaler = joblib.load(mp), joblib.load(sp)
    probs = clf.predict_proba(scaler.transform(features_vec.reshape(1,-1)))[0]
    idx = int(np.argmax(probs)); return LABELS[idx], float(probs[idx])

# -------------------- Lazy import YOLO --------------------
@st.cache_resource
def load_detector():
    try:
        from ultralytics import YOLO
    except Exception as e:
        st.error(
            "Ultralytics å¯¼å…¥å¤±è´¥ã€‚è¯·ç¡®ä¿ requirements.txt åŒ…å«ï¼š\n"
            "opencv-python-headless==4.8.1.78, ultralytics==8.2.103, numpy==1.26.4\n"
            "å¹¶ Clear cache å Rebootã€‚\n"
            f"åŸå§‹å¼‚å¸¸ï¼š{repr(e)}"
        ); st.stop()
    return YOLO("yolov8n.pt")

# -------------------- Summary & explanations --------------------
def interpret_affective_state(segments):
    mean_arousal = np.mean([affect_from_behavior(s.auto_label, s.bark)[0] for s in segments])
    mean_valence = np.mean([affect_from_behavior(s.auto_label, s.bark)[1] for s in segments])
    behavior_counts = {}
    for s in segments: behavior_counts[s.auto_label] = behavior_counts.get(s.auto_label, 0) + 1
    main_behavior = max(behavior_counts, key=behavior_counts.get)
    ar, va = round(mean_arousal,2), round(mean_valence,2)

    if ar > 0.7 and va > 0.6:
        mood, reason, advice = "éå¸¸å…´å¥‹ä¸”æ„‰å¿«", "å¸¸è§äºç©è€/å¥”è·‘æˆ–ä¸ç†Ÿäººäº’åŠ¨ã€‚", "æ­¤æ—¶äº’åŠ¨æˆ–è®­ç»ƒæ•ˆç‡æœ€é«˜ã€‚"
    elif ar < 0.4 and va > 0.6:
        mood, reason, advice = "å¹³é™ä¸”å®‰å…¨", "ç¯å¢ƒå¯é¢„æœŸã€æ— å‹åŠ›åˆºæ¿€ã€‚", "ä¿æŒå®‰ç¨³ç¯å¢ƒä¸è½»æŸ”æŠšè§¦ã€‚"
    elif ar > 0.7 and va < 0.5:
        mood, reason, advice = "ç´§å¼ æˆ–è¿‡åº¦å…´å¥‹", "å¯¹ç¯å¢ƒè¿‡åº¦ååº”ï¼Œæˆ–å­˜åœ¨è½»åº¦ç„¦è™‘ã€‚", "ç”¨ä½å¼ºåº¦æ¸¸æˆè½¬ç§»æ³¨æ„ï¼Œé¿å…çªå‘åˆºæ¿€ã€‚"
    elif ar < 0.4 and va < 0.5:
        mood, reason, advice = "æƒ…ç»ªä½è½æˆ–ç–²æƒ«", "æ´»åŠ¨å‡å°‘ä¸é•¿æ—¶é—´å§èººã€‚", "å…³æ³¨é¥®é£Ÿç¡çœ ï¼Œå¿…è¦æ—¶å¢åŠ è½»åº¦å¤–å‡ºæˆ–ä½“æ£€ã€‚"
    else:
        mood, reason, advice = "ä¸­æ€§å¹³ç¨³", "ä¼‘æ¯ä¸æ´»åŠ¨äº¤æ›¿çš„æ­£å¸¸çŠ¶æ€ã€‚", "ä¿æŒå½“å‰ä½œæ¯å³å¯ã€‚"

    return (
        f"ğŸ• è¿™æ®µè§†é¢‘ä¸­ä»¥ **{main_behavior}** ä¸ºä¸»ï¼›æ•´ä½“æƒ…ç»ª **{mood}**ã€‚\n"
        f"{reason}\n"
        f"ç§‘å­¦æŒ‡æ ‡ï¼šå”¤é†’åº¦ {ar:.2f}ã€æ•ˆä»· {va:.2f}ã€‚\n"
        f"å»ºè®®ï¼š{advice}"
    )

def explain_micro(ev: dict):
    out = []
    if not ev: return out
    if ev.get("wag_mag_mu",0) > 6.0: out.append("å°¾æ‘†å¹…åº¦æ˜æ˜¾ â†’ å”¤é†’åº¦è¾ƒé«˜ï¼ˆå…´å¥‹/ç´§å¼ å¯èƒ½ï¼‰ã€‚")
    if ev.get("ear_up_mu",0) > 0.20: out.append("è€³ä½ä¸Šæ‰¬ â†’ è­¦è§‰/å…³æ³¨åº¦é«˜ã€‚")
    if ev.get("ear_up_mu",1) < 0.10: out.append("è€³ä½æ”¾æ¾/åè´´ â†’ å®‰å…¨æ„Ÿè¾ƒé«˜ã€‚")
    if ev.get("eye_open_mu",1) < 0.28: out.append("çœ¼è£‚è¾ƒå° â†’ æ”¾æ¾æˆ–è½»åº¦ç–²æƒ«ã€‚")
    if ev.get("mouth_open_mu",0) > 0.18: out.append("å£è£‚è¾¹ç¼˜æ´»è·ƒ â†’ å–˜æ°”/ç¼“è§£å‹åŠ›è¡Œä¸ºã€‚")
    return out

# -------------------- Page --------------------
st.set_page_config(page_title=APP_TITLE, layout="centered")
st.title(APP_TITLE)

if cv2 is None:
    st.error("OpenCV æœªæ­£ç¡®åŠ è½½ã€‚è¯·ä½¿ç”¨ opencv-python-headless==4.8.1.78ï¼Œå¹¶ Clear cache å Rebootã€‚")
    if CV2_IMPORT_ERR: st.caption(f"å¯¼å…¥å¼‚å¸¸ï¼š{repr(CV2_IMPORT_ERR)}")
    st.stop()

with st.sidebar:
    st.header("è®¾ç½®")
    pro_mode = st.toggle("é«˜çº§æ¨¡å¼ï¼ˆé¢å‘ä¸“ä¸šç”¨æˆ·ï¼‰", value=False)
    PRESETS = {
        "å®¶åº­å®¤å†…ï¼ˆæ™®é€šï¼‰":       {"conf_th": 0.35, "sample_fps": 6,  "max_seconds": 25},
        "å®¶åº­é™¢å­/æˆ·å¤–ï¼ˆå…‰çº¿è¶³ï¼‰": {"conf_th": 0.30, "sample_fps": 6,  "max_seconds": 25},
        "å¼±å…‰/æ¨¡ç³Šï¼ˆæ›´ç¨³ï¼‰":       {"conf_th": 0.45, "sample_fps": 5,  "max_seconds": 30},
        "è¿åŠ¨å¤šï¼ˆæ›´å¿«ï¼‰":         {"conf_th": 0.35, "sample_fps": 8,  "max_seconds": 20},
    }
    if not pro_mode:
        preset = st.selectbox("åœºæ™¯é¢„è®¾", list(PRESETS.keys()), index=0)
        speed_vs_acc = st.slider("é€Ÿåº¦ â†” å‡†ç¡®åº¦", 0, 10, 6)
        base = PRESETS[preset]
        conf_th   = float(np.clip(base["conf_th"] + (5 - speed_vs_acc) * 0.01, 0.20, 0.55))
        sample_fps = int(np.clip(base["sample_fps"] + (speed_vs_acc - 5) * 0.5, 3, 12))
        max_seconds = int(np.clip(base["max_seconds"] + (5 - speed_vs_acc) * 1.5, 10, 60))
        lowconf_th = 0.65
        st.caption(f"ç­–ç•¥ï¼šé˜ˆå€¼â‰ˆ{conf_th:.2f}ï¼ŒæŠ½å¸§â‰ˆ{sample_fps} fpsï¼Œæœ€é•¿ {max_seconds}sã€‚")
    else:
        max_seconds = st.slider("åˆ†ææ—¶é•¿ä¸Šé™(ç§’)", 5, 120, 25)
        conf_th = st.slider("æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆYOLOï¼‰", 0.1, 0.9, 0.35)
        sample_fps = st.slider("åˆ†ææŠ½å¸§é€Ÿç‡(fps)", 3, 24, 6)
        lowconf_th = st.slider("ä½ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆè§¦å‘æ ‡æ³¨ï¼‰", 0.50, 0.90, 0.65)
        if SK_OK:
            st.markdown("---")
            if st.button("ğŸ§  ä½¿ç”¨å·²æ ‡æ³¨æ ·æœ¬æ”¹è¿›æ¨¡å‹"):
                X_all, y_all = load_samples()
                if X_all is None: st.warning("æš‚æ— æ ‡æ³¨æ ·æœ¬ã€‚å…ˆåœ¨ä¸‹æ–¹æ—¶é—´è½´ä¸­ä¿å­˜å‡ æ¡è®­ç»ƒæ ·æœ¬ã€‚")
                else:
                    _, _, tag = fit_or_partial_update(X_all, y_all)
                    st.success(f"æ¨¡å‹å·²æ›´æ–° âœ…ï¼ˆç‰ˆæœ¬ {tag}ï¼‰")
        else:
            st.info("å¢é‡å­¦ä¹ æœªå¯ç”¨ï¼ˆscikit-learn æœªå®‰è£…ï¼‰ã€‚")

st.caption("ä¸Šä¼ çŸ­è§†é¢‘ï¼Œè¿›è¡Œè¡Œä¸ºä¸æƒ…ç»ªï¼ˆå”¤é†’/æ•ˆä»·ï¼‰æ¨æ–­ï¼›å¹¶è¾“å‡ºâ€œè¯æ®å¡+è‡ªç„¶è¯­è¨€æ€»ç»“â€ã€‚")
uploaded = st.file_uploader("ä¸Šä¼ è§†é¢‘ (mp4/mov/mkv)", type=["mp4","mov","mkv"])

if uploaded:
    tmpf = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
    tmpf.write(uploaded.read()); tmpf.close()

    det = load_detector()
    cap = cv2.VideoCapture(tmpf.name)

    raw_fps = cap.get(cv2.CAP_PROP_FPS) or 30
    if raw_fps < 20: sample_fps = min(sample_fps, 6)

    fps = raw_fps or 30
    total_frames = int(min(cap.get(cv2.CAP_PROP_FRAME_COUNT) or fps*max_seconds, max_seconds*fps))
    step = max(1, int(round(fps / sample_fps)))
    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)); H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    st.info("å¼€å§‹åˆ†æï¼ˆæŠ½å¸§æé€Ÿï¼‰ã€‚")
    progress = st.progress(0.0)

    last_box = None; last_area = None
    segments: List[Segment] = []
    window_frames = max(3, int(sample_fps * 1.2))  # ~1.2s
    buf_feats, buf_times = [], []
    prev_tail_gray = None
    micro_buf = []

    for idx in range(0, total_frames, step):
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if not ret: break
        t_sec = idx / fps

        res = det(frame, conf=conf_th, verbose=False)[0]
        dog_boxes = []
        for b in res.boxes:
            cls = int(b.cls[0].item())
            if det.model.names.get(cls, "") == "dog":
                x1,y1,x2,y2 = b.xyxy[0].cpu().numpy().astype(int).tolist()
                dog_boxes.append([x1,y1,x2,y2, float(b.conf[0].item())])

        if dog_boxes:
            dog_boxes.sort(key=lambda x: (x[2]-x[0])*(x[3]-x[1]), reverse=True)
            box = dog_boxes[0][:4]

            cx, cy = (box[0]+box[2])/2, (box[1]+box[3])/2
            w_box, h_box = box[2]-box[0], box[3]-box[1]
            area = w_box * h_box
            aspect = min(w_box, h_box) / (max(w_box, h_box)+1e-6)

            speed = 0.0; acc = 0.0; area_chg = 0.0
            if last_box is not None and iou(last_box, box) > 0.1:
                lx, ly = (last_box[0]+last_box[2])/2, (last_box[1]+last_box[3])/2
                speed = math.hypot(cx-lx, cy-ly)
                if last_area is not None and last_area > 0:
                    area_chg = abs(area - last_area) / (last_area + 1e-6)
            last_box, last_area = box, area
            if buf_feats:
                prev_speed = buf_feats[-1][0]
                acc = max(0.0, speed - prev_speed)

            features_vec = np.array([speed, acc, aspect, area/(W*H+1e-6), area_chg], dtype=np.float32)
            buf_feats.append((speed, acc, aspect, area/(W*H+1e-6), area_chg))
            buf_times.append(t_sec)

            # --- micro features (head & tail) ---
            head_roi = crop_roi(frame, box, (0.15, 0.00, 0.85, 0.45))
            tail_roi = crop_roi(frame, box, (0.55, 0.60, 1.00, 1.00))
            tail_gray = cv2.cvtColor(tail_roi, cv2.COLOR_BGR2GRAY) if tail_roi is not None else None
            tail_feats = tail_wag_features(prev_tail_gray, tail_gray) if tail_gray is not None else None
            prev_tail_gray = tail_gray
            head_feats = head_micro_features(head_roi)
            micro_buf.append({"t": t_sec, "head": head_feats, "tail": tail_feats})

            # --- segment assemble ---
            if len(buf_feats) >= window_frames:
                f = np.array(buf_feats[-window_frames:], dtype=np.float32)
                agg = np.concatenate([f.mean(axis=0), f.std(axis=0), f.max(axis=0)], axis=0)  # 15 dims
                t0, t1 = buf_times[-window_frames], buf_times[-1]

                rb_label, rb_conf = rule_behavior(float(f[:,0].mean()), float(f[:,2].mean()), float(f[:,4].mean()))
                mdl_pred = predict_with_model(agg) if SK_OK else None
                if mdl_pred is not None:
                    auto_label, auto_conf = mdl_pred
                    if auto_conf < 0.55 and rb_conf > auto_conf:
                        auto_label, auto_conf = rb_label, rb_conf
                else:
                    auto_label, auto_conf = rb_label, rb_conf

                seg = Segment(
                    seg_id=str(uuid.uuid4()),
                    t_start=float(t0), t_end=float(t1),
                    features=agg, auto_label=auto_label, auto_conf=auto_conf,
                    bark=False
                )

                # attach micro-evidence
                win_samples = [m for m in micro_buf if m["t"] >= t0-1e-3 and m["t"] <= t1+1e-3]
                def agg_stats(key):
                    vals = [s["head"][key] for s in win_samples if s["head"] and s["head"].get(key) is not None]
                    return (float(np.mean(vals)) if vals else None, float(np.std(vals)) if vals else None)
                eye_mu,_ = agg_stats("eye_open"); mouth_mu,_ = agg_stats("mouth_open"); ear_mu,_ = agg_stats("ear_up")
                wag_mag = [s["tail"]["wag_mag"] for s in win_samples if s["tail"] and s["tail"].get("wag_mag") is not None]
                wag_or  = [s["tail"]["wag_orient"] for s in win_samples if s["tail"] and s["tail"].get("wag_orient") is not None]
                seg.micro = {
                    "eye_open_mu": (None if eye_mu is None else float(eye_mu)),
                    "mouth_open_mu": (None if mouth_mu is None else float(mouth_mu)),
                    "ear_up_mu": (None if ear_mu is None else float(ear_mu)),
                    "wag_mag_mu": (None if not wag_mag else float(np.mean(wag_mag))),
                    "wag_orient_mu": (None if not wag_or else float(np.mean(wag_or))),
                }
                segments.append(seg)

        progress.progress(min(1.0, (idx+step)/max(1,total_frames)))

    cap.release()

    if not segments:
        st.error("æœªæ£€æµ‹åˆ°ç‹—ã€‚è¯·ç¡®ä¿ç”»é¢ä¸­æœ‰æ¸…æ™°çš„ç‹—å¹¶æœ‰è¶³å¤Ÿè¿åŠ¨ã€‚")
        st.stop()

    # ---------- Table ----------
    st.subheader("åˆ†æç»“æœï¼ˆæ—¶é—´è½´ï¼‰")
    rows = []
    for s in segments:
        a,v,_ = affect_from_behavior(s.auto_label, s.bark)
        rows.append({
            "start(s)": round(s.t_start,2), "end(s)": round(s.t_end,2),
            "behavior": s.auto_label, "conf": round(s.auto_conf,2),
            "arousal": round(a,2), "valence": round(v,2)
        })
    st.dataframe(rows, use_container_width=True)

    # ---------- Evidence cards ----------
    st.subheader("ğŸ§  å¾®è¡¨æƒ…ä¸å°¾éƒ¨è¯æ®å¡ï¼ˆè‡ªåŠ¨è§£é‡Šï¼‰")
    for s in segments:
        ev = getattr(s, "micro", None)
        if not ev: continue
        explain = explain_micro(ev) or ["æœªå‘ç°æ˜¾è‘—å¾®è¡¨æƒ…ä¿¡å·ï¼Œæ•´ä½“å¤„äºä¸­æ€§èŒƒå›´ã€‚"]
        with st.expander(f"{s.t_start:.2f}â€“{s.t_end:.2f}s  è¯æ®ä¸è§£é‡Šï¼ˆ{s.auto_label}ï¼Œ{s.auto_conf:.2f}ï¼‰"):
            st.write("\n".join(f"- {t}" for t in explain))
            st.json(ev)

    # ---------- NL summary ----------
    st.subheader("ğŸ§© è¡Œä¸ºå¿ƒç†æ€»ç»“")
    summary_text = interpret_affective_state(segments)
    st.info(summary_text)

    # ---------- Reports ----------
    if st.button("ğŸ“„ å¯¼å‡ºæœ¬æ¬¡æŠ¥å‘Š(JSON+TXT)"):
        rid = time.strftime("%Y%m%d_%H%M%S")
        report = {
            "video": uploaded.name,
            "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            "summary_text": summary_text,
            "segments_detailed": [
                {
                    "t_start": s.t_start, "t_end": s.t_end,
                    "behavior": s.auto_label, "conf": s.auto_conf,
                    "affect": {
                        "arousal": affect_from_behavior(s.auto_label, False)[0],
                        "valence": affect_from_behavior(s.auto_label, False)[1]
                    },
                    "micro_evidence": getattr(s, "micro", None),
                    "micro_explain": explain_micro(getattr(s, "micro", None)),
                } for s in segments
            ]
        }
        json_path = os.path.join(REPORT_DIR, f"report_{rid}.json")
        with open(json_path, "w", encoding="utf-8") as f: json.dump(report, f, indent=2)

        txt_lines = [summary_text, "", "â€”â€” ç»†èŠ‚è¯æ® â€”â€”"]
        for sd in report["segments_detailed"]:
            txt_lines.append(f"[{sd['t_start']:.1f}-{sd['t_end']:.1f}s] {sd['behavior']} (conf={sd['conf']:.2f})")
            if sd["micro_explain"]:
                for t in sd["micro_explain"]: txt_lines.append(f"  Â· {t}")
            else:
                txt_lines.append("  Â· æ— æ˜¾è‘—å¾®è¡¨æƒ…ä¿¡å·")
        txt_path = os.path.join(REPORT_DIR, f"report_{rid}.txt")
        with open(txt_path, "w", encoding="utf-8") as f: f.write("\n".join(txt_lines))

        st.success(f"å·²å¯¼å‡ºï¼š{json_path}  å’Œ  {txt_path}")

st.markdown("---")
st.caption("Cloud å®‰å…¨ç‰ˆï¼šOpenCV headlessï¼ŒéŸ³é¢‘å…³é—­ï¼›å®‰è£… scikit-learn åè‡ªåŠ¨å¯ç”¨å¢é‡å­¦ä¹ æŒ‰é’®ã€‚")
